"use strict";
// *** WARNING: this file was generated by pulumi-language-nodejs. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***
Object.defineProperty(exports, "__esModule", { value: true });
exports.AiEndpointWithModelGardenDeployment = void 0;
const pulumi = require("@pulumi/pulumi");
const utilities = require("../utilities");
/**
 * Create an Endpoint and deploy a Model Garden model to it.
 *
 * To get more information about EndpointWithModelGardenDeployment, see:
 *
 * * [API documentation](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations/deploy)
 * * How-to Guides
 *     * [Overview of Model Garden](https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models)
 *     * [Overview of self-deployed models](https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/self-deployed-models)
 *     * [Use models in Model Garden](https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/use-models)
 *
 * ## Example Usage
 *
 * ### Vertex Ai Deploy Basic
 *
 * ```typescript
 * import * as pulumi from "@pulumi/pulumi";
 * import * as gcp from "@pulumi/gcp";
 *
 * const deploy = new gcp.vertex.AiEndpointWithModelGardenDeployment("deploy", {
 *     publisherModelName: "publishers/google/models/paligemma@paligemma-224-float32",
 *     location: "us-central1",
 *     modelConfig: {
 *         acceptEula: true,
 *     },
 * });
 * ```
 * ### Vertex Ai Deploy Huggingface Model
 *
 * ```typescript
 * import * as pulumi from "@pulumi/pulumi";
 * import * as gcp from "@pulumi/gcp";
 *
 * const deploy = new gcp.vertex.AiEndpointWithModelGardenDeployment("deploy", {
 *     huggingFaceModelId: "Qwen/Qwen3-0.6B",
 *     location: "us-central1",
 *     modelConfig: {
 *         acceptEula: true,
 *     },
 * });
 * ```
 * ### Vertex Ai Deploy With Configs
 *
 * ```typescript
 * import * as pulumi from "@pulumi/pulumi";
 * import * as gcp from "@pulumi/gcp";
 *
 * const deploy = new gcp.vertex.AiEndpointWithModelGardenDeployment("deploy", {
 *     publisherModelName: "publishers/google/models/paligemma@paligemma-224-float32",
 *     location: "us-central1",
 *     modelConfig: {
 *         acceptEula: true,
 *     },
 *     deployConfig: {
 *         dedicatedResources: {
 *             machineSpec: {
 *                 machineType: "g2-standard-16",
 *                 acceleratorType: "NVIDIA_L4",
 *                 acceleratorCount: 1,
 *             },
 *             minReplicaCount: 1,
 *         },
 *     },
 * });
 * ```
 * ### Vertex Ai Deploy Multiple Models In Parallel
 *
 * ```typescript
 * import * as pulumi from "@pulumi/pulumi";
 * import * as gcp from "@pulumi/gcp";
 *
 * const deploy_gemma_11_2b_it = new gcp.vertex.AiEndpointWithModelGardenDeployment("deploy-gemma-1_1-2b-it", {
 *     publisherModelName: "publishers/google/models/gemma@gemma-1.1-2b-it",
 *     location: "us-central1",
 *     modelConfig: {
 *         acceptEula: true,
 *     },
 *     deployConfig: {
 *         dedicatedResources: {
 *             machineSpec: {
 *                 machineType: "g2-standard-12",
 *                 acceleratorType: "us-central1",
 *                 acceleratorCount: 1,
 *             },
 *             minReplicaCount: 1,
 *         },
 *     },
 * });
 * const deploy_qwen3_06b = new gcp.vertex.AiEndpointWithModelGardenDeployment("deploy-qwen3-0_6b", {
 *     huggingFaceModelId: "Qwen/Qwen3-0.6B",
 *     location: "us-central1",
 *     modelConfig: {
 *         acceptEula: true,
 *     },
 *     deployConfig: {
 *         dedicatedResources: {
 *             machineSpec: {
 *                 machineType: "g2-standard-12",
 *                 acceleratorType: "NVIDIA_L4",
 *                 acceleratorCount: 1,
 *             },
 *             minReplicaCount: 1,
 *         },
 *     },
 * });
 * const deploy_llama_32_1b = new gcp.vertex.AiEndpointWithModelGardenDeployment("deploy-llama-3_2-1b", {
 *     publisherModelName: "publishers/meta/models/llama3-2@llama-3.2-1b",
 *     location: "us-central1",
 *     modelConfig: {
 *         acceptEula: true,
 *     },
 *     deployConfig: {
 *         dedicatedResources: {
 *             machineSpec: {
 *                 machineType: "g2-standard-12",
 *                 acceleratorType: "NVIDIA_L4",
 *                 acceleratorCount: 1,
 *             },
 *             minReplicaCount: 1,
 *         },
 *     },
 * });
 * ```
 * ### Vertex Ai Deploy Multiple Models In Sequence
 *
 * ```typescript
 * import * as pulumi from "@pulumi/pulumi";
 * import * as gcp from "@pulumi/gcp";
 *
 * const deploy_gemma_11_2b_it = new gcp.vertex.AiEndpointWithModelGardenDeployment("deploy-gemma-1_1-2b-it", {
 *     publisherModelName: "publishers/google/models/gemma@gemma-1.1-2b-it",
 *     location: "us-central1",
 *     modelConfig: {
 *         acceptEula: true,
 *     },
 *     deployConfig: {
 *         dedicatedResources: {
 *             machineSpec: {
 *                 machineType: "g2-standard-12",
 *                 acceleratorType: "NVIDIA_L4",
 *                 acceleratorCount: 1,
 *             },
 *             minReplicaCount: 1,
 *         },
 *     },
 * });
 * const deploy_qwen3_06b = new gcp.vertex.AiEndpointWithModelGardenDeployment("deploy-qwen3-0_6b", {
 *     huggingFaceModelId: "Qwen/Qwen3-0.6B",
 *     location: "us-central1",
 *     modelConfig: {
 *         acceptEula: true,
 *     },
 *     deployConfig: {
 *         dedicatedResources: {
 *             machineSpec: {
 *                 machineType: "g2-standard-12",
 *                 acceleratorType: "NVIDIA_L4",
 *                 acceleratorCount: 1,
 *             },
 *             minReplicaCount: 1,
 *         },
 *     },
 * }, {
 *     dependsOn: [deploy_gemma_11_2b_it],
 * });
 * const deploy_llama_32_1b = new gcp.vertex.AiEndpointWithModelGardenDeployment("deploy-llama-3_2-1b", {
 *     publisherModelName: "publishers/meta/models/llama3-2@llama-3.2-1b",
 *     location: "us-central1",
 *     modelConfig: {
 *         acceptEula: true,
 *     },
 *     deployConfig: {
 *         dedicatedResources: {
 *             machineSpec: {
 *                 machineType: "g2-standard-12",
 *                 acceleratorType: "NVIDIA_L4",
 *                 acceleratorCount: 1,
 *             },
 *             minReplicaCount: 1,
 *         },
 *     },
 * }, {
 *     dependsOn: [deploy_qwen3_06b],
 * });
 * ```
 *
 * ## Import
 *
 * This resource does not support import.
 */
class AiEndpointWithModelGardenDeployment extends pulumi.CustomResource {
    /**
     * Get an existing AiEndpointWithModelGardenDeployment resource's state with the given name, ID, and optional extra
     * properties used to qualify the lookup.
     *
     * @param name The _unique_ name of the resulting resource.
     * @param id The _unique_ provider ID of the resource to lookup.
     * @param state Any extra arguments used during the lookup.
     * @param opts Optional settings to control the behavior of the CustomResource.
     */
    static get(name, id, state, opts) {
        return new AiEndpointWithModelGardenDeployment(name, state, Object.assign(Object.assign({}, opts), { id: id }));
    }
    /**
     * Returns true if the given object is an instance of AiEndpointWithModelGardenDeployment.  This is designed to work even
     * when multiple copies of the Pulumi SDK have been loaded into the same process.
     */
    static isInstance(obj) {
        if (obj === undefined || obj === null) {
            return false;
        }
        return obj['__pulumiType'] === AiEndpointWithModelGardenDeployment.__pulumiType;
    }
    constructor(name, argsOrState, opts) {
        let resourceInputs = {};
        opts = opts || {};
        if (opts.id) {
            const state = argsOrState;
            resourceInputs["deployConfig"] = state ? state.deployConfig : undefined;
            resourceInputs["deployedModelDisplayName"] = state ? state.deployedModelDisplayName : undefined;
            resourceInputs["deployedModelId"] = state ? state.deployedModelId : undefined;
            resourceInputs["endpoint"] = state ? state.endpoint : undefined;
            resourceInputs["endpointConfig"] = state ? state.endpointConfig : undefined;
            resourceInputs["huggingFaceModelId"] = state ? state.huggingFaceModelId : undefined;
            resourceInputs["location"] = state ? state.location : undefined;
            resourceInputs["modelConfig"] = state ? state.modelConfig : undefined;
            resourceInputs["project"] = state ? state.project : undefined;
            resourceInputs["publisherModelName"] = state ? state.publisherModelName : undefined;
        }
        else {
            const args = argsOrState;
            if ((!args || args.location === undefined) && !opts.urn) {
                throw new Error("Missing required property 'location'");
            }
            resourceInputs["deployConfig"] = args ? args.deployConfig : undefined;
            resourceInputs["endpointConfig"] = args ? args.endpointConfig : undefined;
            resourceInputs["huggingFaceModelId"] = args ? args.huggingFaceModelId : undefined;
            resourceInputs["location"] = args ? args.location : undefined;
            resourceInputs["modelConfig"] = args ? args.modelConfig : undefined;
            resourceInputs["project"] = args ? args.project : undefined;
            resourceInputs["publisherModelName"] = args ? args.publisherModelName : undefined;
            resourceInputs["deployedModelDisplayName"] = undefined /*out*/;
            resourceInputs["deployedModelId"] = undefined /*out*/;
            resourceInputs["endpoint"] = undefined /*out*/;
        }
        opts = pulumi.mergeOptions(utilities.resourceOptsDefaults(), opts);
        super(AiEndpointWithModelGardenDeployment.__pulumiType, name, resourceInputs, opts);
    }
}
exports.AiEndpointWithModelGardenDeployment = AiEndpointWithModelGardenDeployment;
/** @internal */
AiEndpointWithModelGardenDeployment.__pulumiType = 'gcp:vertex/aiEndpointWithModelGardenDeployment:AiEndpointWithModelGardenDeployment';
//# sourceMappingURL=aiEndpointWithModelGardenDeployment.js.map