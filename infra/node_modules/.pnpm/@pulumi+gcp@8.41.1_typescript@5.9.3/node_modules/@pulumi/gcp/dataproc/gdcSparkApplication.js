"use strict";
// *** WARNING: this file was generated by pulumi-language-nodejs. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***
Object.defineProperty(exports, "__esModule", { value: true });
exports.GdcSparkApplication = void 0;
const pulumi = require("@pulumi/pulumi");
const utilities = require("../utilities");
/**
 * A Spark application is a single Spark workload run on a GDC cluster.
 *
 * To get more information about SparkApplication, see:
 *
 * * [API documentation](https://cloud.google.com/dataproc-gdc/docs/reference/rest/v1/projects.locations.serviceInstances.sparkApplications)
 * * How-to Guides
 *     * [Dataproc Intro](https://cloud.google.com/dataproc/)
 *
 * ## Example Usage
 *
 * ### Dataprocgdc Sparkapplication Basic
 *
 * ```typescript
 * import * as pulumi from "@pulumi/pulumi";
 * import * as gcp from "@pulumi/gcp";
 *
 * const spark_application = new gcp.dataproc.GdcSparkApplication("spark-application", {
 *     sparkApplicationId: "tf-e2e-spark-app-basic",
 *     serviceinstance: "do-not-delete-dataproc-gdc-instance",
 *     project: "my-project",
 *     location: "us-west2",
 *     namespace: "default",
 *     sparkApplicationConfig: {
 *         mainClass: "org.apache.spark.examples.SparkPi",
 *         jarFileUris: ["file:///usr/lib/spark/examples/jars/spark-examples.jar"],
 *         args: ["10000"],
 *     },
 * });
 * ```
 * ### Dataprocgdc Sparkapplication
 *
 * ```typescript
 * import * as pulumi from "@pulumi/pulumi";
 * import * as gcp from "@pulumi/gcp";
 *
 * const appEnv = new gcp.dataproc.GdcApplicationEnvironment("app_env", {
 *     applicationEnvironmentId: "tf-e2e-spark-app-env",
 *     serviceinstance: "do-not-delete-dataproc-gdc-instance",
 *     project: "my-project",
 *     location: "us-west2",
 *     namespace: "default",
 * });
 * const spark_application = new gcp.dataproc.GdcSparkApplication("spark-application", {
 *     sparkApplicationId: "tf-e2e-spark-app",
 *     serviceinstance: "do-not-delete-dataproc-gdc-instance",
 *     project: "my-project",
 *     location: "us-west2",
 *     namespace: "default",
 *     labels: {
 *         "test-label": "label-value",
 *     },
 *     annotations: {
 *         an_annotation: "annotation_value",
 *     },
 *     properties: {
 *         "spark.executor.instances": "2",
 *     },
 *     applicationEnvironment: appEnv.name,
 *     version: "1.2",
 *     sparkApplicationConfig: {
 *         mainJarFileUri: "file:///usr/lib/spark/examples/jars/spark-examples.jar",
 *         jarFileUris: ["file:///usr/lib/spark/examples/jars/spark-examples.jar"],
 *         archiveUris: ["file://usr/lib/spark/examples/spark-examples.jar"],
 *         fileUris: ["file:///usr/lib/spark/examples/jars/spark-examples.jar"],
 *     },
 * });
 * ```
 * ### Dataprocgdc Sparkapplication Pyspark
 *
 * ```typescript
 * import * as pulumi from "@pulumi/pulumi";
 * import * as gcp from "@pulumi/gcp";
 *
 * const spark_application = new gcp.dataproc.GdcSparkApplication("spark-application", {
 *     sparkApplicationId: "tf-e2e-pyspark-app",
 *     serviceinstance: "do-not-delete-dataproc-gdc-instance",
 *     project: "my-project",
 *     location: "us-west2",
 *     namespace: "default",
 *     displayName: "A Pyspark application for a Terraform create test",
 *     dependencyImages: ["gcr.io/some/image"],
 *     pysparkApplicationConfig: {
 *         mainPythonFileUri: "gs://goog-dataproc-initialization-actions-us-west2/conda/test_conda.py",
 *         jarFileUris: ["file:///usr/lib/spark/examples/jars/spark-examples.jar"],
 *         pythonFileUris: ["gs://goog-dataproc-initialization-actions-us-west2/conda/get-sys-exec.py"],
 *         fileUris: ["file://usr/lib/spark/examples/spark-examples.jar"],
 *         archiveUris: ["file://usr/lib/spark/examples/spark-examples.jar"],
 *         args: ["10"],
 *     },
 * });
 * ```
 * ### Dataprocgdc Sparkapplication Sparkr
 *
 * ```typescript
 * import * as pulumi from "@pulumi/pulumi";
 * import * as gcp from "@pulumi/gcp";
 *
 * const spark_application = new gcp.dataproc.GdcSparkApplication("spark-application", {
 *     sparkApplicationId: "tf-e2e-sparkr-app",
 *     serviceinstance: "do-not-delete-dataproc-gdc-instance",
 *     project: "my-project",
 *     location: "us-west2",
 *     namespace: "default",
 *     displayName: "A SparkR application for a Terraform create test",
 *     sparkRApplicationConfig: {
 *         mainRFileUri: "gs://some-bucket/something.R",
 *         fileUris: ["file://usr/lib/spark/examples/spark-examples.jar"],
 *         archiveUris: ["file://usr/lib/spark/examples/spark-examples.jar"],
 *         args: ["10"],
 *     },
 * });
 * ```
 * ### Dataprocgdc Sparkapplication Sparksql
 *
 * ```typescript
 * import * as pulumi from "@pulumi/pulumi";
 * import * as gcp from "@pulumi/gcp";
 *
 * const spark_application = new gcp.dataproc.GdcSparkApplication("spark-application", {
 *     sparkApplicationId: "tf-e2e-sparksql-app",
 *     serviceinstance: "do-not-delete-dataproc-gdc-instance",
 *     project: "my-project",
 *     location: "us-west2",
 *     namespace: "default",
 *     displayName: "A SparkSql application for a Terraform create test",
 *     sparkSqlApplicationConfig: {
 *         jarFileUris: ["file:///usr/lib/spark/examples/jars/spark-examples.jar"],
 *         queryList: {
 *             queries: ["show tables;"],
 *         },
 *         scriptVariables: {
 *             MY_VAR: "1",
 *         },
 *     },
 * });
 * ```
 * ### Dataprocgdc Sparkapplication Sparksql Query File
 *
 * ```typescript
 * import * as pulumi from "@pulumi/pulumi";
 * import * as gcp from "@pulumi/gcp";
 *
 * const spark_application = new gcp.dataproc.GdcSparkApplication("spark-application", {
 *     sparkApplicationId: "tf-e2e-sparksql-app",
 *     serviceinstance: "do-not-delete-dataproc-gdc-instance",
 *     project: "my-project",
 *     location: "us-west2",
 *     namespace: "default",
 *     displayName: "A SparkSql application for a Terraform create test",
 *     sparkSqlApplicationConfig: {
 *         jarFileUris: ["file:///usr/lib/spark/examples/jars/spark-examples.jar"],
 *         queryFileUri: "gs://some-bucket/something.sql",
 *         scriptVariables: {
 *             MY_VAR: "1",
 *         },
 *     },
 * });
 * ```
 *
 * ## Import
 *
 * SparkApplication can be imported using any of these accepted formats:
 *
 * * `projects/{{project}}/locations/{{location}}/serviceInstances/{{serviceinstance}}/sparkApplications/{{spark_application_id}}`
 *
 * * `{{project}}/{{location}}/{{serviceinstance}}/{{spark_application_id}}`
 *
 * * `{{location}}/{{serviceinstance}}/{{spark_application_id}}`
 *
 * When using the `pulumi import` command, SparkApplication can be imported using one of the formats above. For example:
 *
 * ```sh
 * $ pulumi import gcp:dataproc/gdcSparkApplication:GdcSparkApplication default projects/{{project}}/locations/{{location}}/serviceInstances/{{serviceinstance}}/sparkApplications/{{spark_application_id}}
 * ```
 *
 * ```sh
 * $ pulumi import gcp:dataproc/gdcSparkApplication:GdcSparkApplication default {{project}}/{{location}}/{{serviceinstance}}/{{spark_application_id}}
 * ```
 *
 * ```sh
 * $ pulumi import gcp:dataproc/gdcSparkApplication:GdcSparkApplication default {{location}}/{{serviceinstance}}/{{spark_application_id}}
 * ```
 */
class GdcSparkApplication extends pulumi.CustomResource {
    /**
     * Get an existing GdcSparkApplication resource's state with the given name, ID, and optional extra
     * properties used to qualify the lookup.
     *
     * @param name The _unique_ name of the resulting resource.
     * @param id The _unique_ provider ID of the resource to lookup.
     * @param state Any extra arguments used during the lookup.
     * @param opts Optional settings to control the behavior of the CustomResource.
     */
    static get(name, id, state, opts) {
        return new GdcSparkApplication(name, state, Object.assign(Object.assign({}, opts), { id: id }));
    }
    /**
     * Returns true if the given object is an instance of GdcSparkApplication.  This is designed to work even
     * when multiple copies of the Pulumi SDK have been loaded into the same process.
     */
    static isInstance(obj) {
        if (obj === undefined || obj === null) {
            return false;
        }
        return obj['__pulumiType'] === GdcSparkApplication.__pulumiType;
    }
    constructor(name, argsOrState, opts) {
        let resourceInputs = {};
        opts = opts || {};
        if (opts.id) {
            const state = argsOrState;
            resourceInputs["annotations"] = state ? state.annotations : undefined;
            resourceInputs["applicationEnvironment"] = state ? state.applicationEnvironment : undefined;
            resourceInputs["createTime"] = state ? state.createTime : undefined;
            resourceInputs["dependencyImages"] = state ? state.dependencyImages : undefined;
            resourceInputs["displayName"] = state ? state.displayName : undefined;
            resourceInputs["effectiveAnnotations"] = state ? state.effectiveAnnotations : undefined;
            resourceInputs["effectiveLabels"] = state ? state.effectiveLabels : undefined;
            resourceInputs["labels"] = state ? state.labels : undefined;
            resourceInputs["location"] = state ? state.location : undefined;
            resourceInputs["monitoringEndpoint"] = state ? state.monitoringEndpoint : undefined;
            resourceInputs["name"] = state ? state.name : undefined;
            resourceInputs["namespace"] = state ? state.namespace : undefined;
            resourceInputs["outputUri"] = state ? state.outputUri : undefined;
            resourceInputs["project"] = state ? state.project : undefined;
            resourceInputs["properties"] = state ? state.properties : undefined;
            resourceInputs["pulumiLabels"] = state ? state.pulumiLabels : undefined;
            resourceInputs["pysparkApplicationConfig"] = state ? state.pysparkApplicationConfig : undefined;
            resourceInputs["reconciling"] = state ? state.reconciling : undefined;
            resourceInputs["serviceinstance"] = state ? state.serviceinstance : undefined;
            resourceInputs["sparkApplicationConfig"] = state ? state.sparkApplicationConfig : undefined;
            resourceInputs["sparkApplicationId"] = state ? state.sparkApplicationId : undefined;
            resourceInputs["sparkRApplicationConfig"] = state ? state.sparkRApplicationConfig : undefined;
            resourceInputs["sparkSqlApplicationConfig"] = state ? state.sparkSqlApplicationConfig : undefined;
            resourceInputs["state"] = state ? state.state : undefined;
            resourceInputs["stateMessage"] = state ? state.stateMessage : undefined;
            resourceInputs["uid"] = state ? state.uid : undefined;
            resourceInputs["updateTime"] = state ? state.updateTime : undefined;
            resourceInputs["version"] = state ? state.version : undefined;
        }
        else {
            const args = argsOrState;
            if ((!args || args.location === undefined) && !opts.urn) {
                throw new Error("Missing required property 'location'");
            }
            if ((!args || args.serviceinstance === undefined) && !opts.urn) {
                throw new Error("Missing required property 'serviceinstance'");
            }
            if ((!args || args.sparkApplicationId === undefined) && !opts.urn) {
                throw new Error("Missing required property 'sparkApplicationId'");
            }
            resourceInputs["annotations"] = args ? args.annotations : undefined;
            resourceInputs["applicationEnvironment"] = args ? args.applicationEnvironment : undefined;
            resourceInputs["dependencyImages"] = args ? args.dependencyImages : undefined;
            resourceInputs["displayName"] = args ? args.displayName : undefined;
            resourceInputs["labels"] = args ? args.labels : undefined;
            resourceInputs["location"] = args ? args.location : undefined;
            resourceInputs["namespace"] = args ? args.namespace : undefined;
            resourceInputs["project"] = args ? args.project : undefined;
            resourceInputs["properties"] = args ? args.properties : undefined;
            resourceInputs["pysparkApplicationConfig"] = args ? args.pysparkApplicationConfig : undefined;
            resourceInputs["serviceinstance"] = args ? args.serviceinstance : undefined;
            resourceInputs["sparkApplicationConfig"] = args ? args.sparkApplicationConfig : undefined;
            resourceInputs["sparkApplicationId"] = args ? args.sparkApplicationId : undefined;
            resourceInputs["sparkRApplicationConfig"] = args ? args.sparkRApplicationConfig : undefined;
            resourceInputs["sparkSqlApplicationConfig"] = args ? args.sparkSqlApplicationConfig : undefined;
            resourceInputs["version"] = args ? args.version : undefined;
            resourceInputs["createTime"] = undefined /*out*/;
            resourceInputs["effectiveAnnotations"] = undefined /*out*/;
            resourceInputs["effectiveLabels"] = undefined /*out*/;
            resourceInputs["monitoringEndpoint"] = undefined /*out*/;
            resourceInputs["name"] = undefined /*out*/;
            resourceInputs["outputUri"] = undefined /*out*/;
            resourceInputs["pulumiLabels"] = undefined /*out*/;
            resourceInputs["reconciling"] = undefined /*out*/;
            resourceInputs["state"] = undefined /*out*/;
            resourceInputs["stateMessage"] = undefined /*out*/;
            resourceInputs["uid"] = undefined /*out*/;
            resourceInputs["updateTime"] = undefined /*out*/;
        }
        opts = pulumi.mergeOptions(utilities.resourceOptsDefaults(), opts);
        const secretOpts = { additionalSecretOutputs: ["effectiveLabels", "pulumiLabels"] };
        opts = pulumi.mergeOptions(opts, secretOpts);
        super(GdcSparkApplication.__pulumiType, name, resourceInputs, opts);
    }
}
exports.GdcSparkApplication = GdcSparkApplication;
/** @internal */
GdcSparkApplication.__pulumiType = 'gcp:dataproc/gdcSparkApplication:GdcSparkApplication';
//# sourceMappingURL=gdcSparkApplication.js.map