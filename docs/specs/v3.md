# Brief - V3 Specification

> Fill in the gaps, keep it brief.

V3 builds on the [V2 spec](./v2.md) with a conversational voice agent powered by LiveKit that replaces the record-then-transcribe flow.

## New Features

### 1. LiveKit Voice Agent

Replace the current record-then-transcribe flow with a real-time conversational AI agent that interviews the user through their weekly update.

**How it works:**

- User opens a voice session (from the web app directly)
- A LiveKit voice agent greets them and asks the 4 core questions one by one
- The user answers conversationally — the agent listens, acknowledges, and moves on
- If an answer is vague, the agent asks a brief follow-up to get specifics
- After all 4 questions, the agent summarizes what it heard and asks if anything is missing
- The session ends and a structured report is generated from the conversation transcript

**The 4 questions (agent asks these conversationally):**

1. "What did you get done this week?"
2. "Any challenges? How did you handle them?"
3. "What's the plan for next week?"
4. "How's the client or stakeholder feeling?"

**Architecture:**

```
Browser (LiveKit Client SDK)
    ↕ WebRTC audio
LiveKit Server (LiveKit Cloud or self-hosted)
    ↕
LiveKit Agents Framework (Python)
    ├── STT: Deepgram (or OpenAI Whisper)
    ├── LLM: OpenAI GPT-4o (conversation logic)
    └── TTS: OpenAI TTS (or ElevenLabs)
```

**Agent behaviour:**

- Warm, concise, professional tone — not robotic, not overly chatty
- Keeps the conversation moving — each question should take 30–60 seconds
- Entire session targets under 5 minutes
- If the user goes off-topic, gently steers back
- Handles interruptions gracefully (barge-in support via LiveKit)
- The agent has context about who the user is (name, team) passed via room metadata

### 2. Session Resilience & Resume

The voice session is designed for real-world interruptions — phone calls, lost signal, closing the tab — to never lose progress.

**State saving:**

- The agent saves conversation state to the backend after each completed question-answer exchange
- State includes: which questions have been answered, the transcript so far, and any extracted data
- State is persisted server-side, keyed by `sessionId`

**Session state model:**

```
{
  sessionId: "abc123",
  userId: "chris@palindrom.ai",
  weekOf: "2025-01-27",
  status: "in_progress",      // pending | in_progress | completed
  currentQuestion: 3,          // 1-indexed, next question to ask
  answers: [
    { question: 1, transcript: "...", completedAt: "..." },
    { question: 2, transcript: "...", completedAt: "..." }
  ],
  fullTranscript: "...",       // running transcript of entire conversation
  createdAt: "...",
  lastActiveAt: "..."          // updated on each interaction
}
```

**Resume flow:**

1. User disconnects mid-session (phone call, signal loss, closed tab, crash)
2. The session remains in `in_progress` state on the server
3. User returns to the same URL (bookmarked or from their dashboard)
4. Backend sees the session exists and is resumable
5. A new LiveKit room is created, agent joins with the saved state as context
6. Agent greets them back: *"Hey Chris, welcome back. We covered your accomplishments and challenges — let's pick up with what's planned for next week."*
7. Conversation continues from where it left off

**Resume window:**

- Sessions are resumable for 24 hours
- After 24 hours, the session expires — user needs to start a new one
- If a session has answers for all 4 questions but wasn't formally completed, it auto-generates the report on expiry (so partial work is never lost)

**What triggers a save:**

- Agent completes a question-answer exchange → save after each one
- Agent detects the user's answer is substantive enough to record
- User explicitly says "hold on" or "I need to pause" → agent acknowledges and saves
- WebRTC connection drops → last state is already saved (saves happen after each Q&A, not on disconnect)

### 3. Voice-First Review & Editing

After the agent interview, the user stays in a voice-driven experience for reviewing and editing their report. No reading walls of text on a phone screen.

**AI read-back:**

Once the report is generated, the agent reads it back to the user aloud using TTS. The user picks a playback speed before it starts:

- **1x** — normal pace, good for careful review
- **1.5x** — default, natural but brisk
- **2x** — speed-run, for people who just want to confirm nothing's wrong

The speed selector is a simple 3-button toggle on screen. The agent reads the full structured report section by section:

> *"Here's your update. TL;DR: You shipped the new onboarding flow and resolved the API latency issue. This week — you completed the onboarding redesign, paired with Faycal on the payments integration, and fixed the P1 latency bug in the search endpoint..."*

**Voice editing:**

After read-back (or at any point — the user can interrupt), the user edits by talking:

- *"Change the TL;DR to mention the client demo"*
- *"Remove the part about the latency bug, that was last week"*
- *"Add that I'm blocked on the design review from Jon"*
- *"Change status from On Track to At Risk"*

The agent processes the instruction, updates the report, and reads back just the changed section for confirmation. This loops until the user says they're happy.

**Voice commands during review:**

| Command | Action |
|---------|--------|
| "Read it again" | Full read-back at current speed |
| "Read the challenges section" | Read-back of a specific section |
| "Slower" / "Faster" | Adjust playback speed |
| "Change [section] to [content]" | Edit a specific section |
| "Add [content] to [section]" | Append to a section |
| "Remove [content]" | Delete from report |
| "Sounds good" / "Save it" | Confirm and save to Notion |
| "Start over" | Regenerate report from transcript |

**Flow:**

```
Agent interview complete
    ↓
"Here's your report. Want me to read it back?"
    ↓
User picks speed (1x / 1.5x / 2x)
    ↓
Agent reads report aloud
    ↓
"Any changes?"
    ↓
User speaks edits (or says "sounds good")
    ↓
Agent reads back changed sections
    ↓
Loop until confirmed
    ↓
Save to Notion
```

The entire flow — from opening the session to saved report — can be done without ever looking at the screen.

**After the call:**

- The full conversation transcript is processed by GPT-4o to extract the structured report (same format as v1/v2: TL;DR, This Week, Challenges, Next Week, Client Pulse, Status)
- User sees the generated report and can refine it (existing refinement flow)
- One-click save to Notion (existing flow)

## Updated Architecture

```
┌──────────────────┐     ┌─────────────────┐
│  Brief Backend    │────→│  LiveKit Cloud  │
│  (Fastify API)    │     │  (WebRTC SFU)   │
└──────────────────┘     └─────────────────┘
         │                         │
         │                         ↕
         │                  ┌─────────────────┐
         │                  │  LiveKit Agent   │
         │                  │  (Python)        │
         │                  │  ├── STT         │
         │                  │  ├── LLM         │
         │                  │  └── TTS         │
         │                  └─────────────────┘
         │
   ┌─────┼───────┐
   │     │       │
┌──┴───┐ ┌┴────┐ ┌┴───────┐
│Notion│ │OpenAI│ │ Clerk  │
│(save)│ │(LLM) │ │ (auth) │
└──────┘ └──────┘ └────────┘
```

**Flow:**

```
1. User opens /call/new from web app (Clerk authenticated)
2. Backend creates session + LiveKit room, returns connection info
3. Frontend connects to LiveKit room via WebRTC
4. LiveKit Agent joins room, starts conversation
5. Agent asks 4 questions, user answers by voice
6. Session ends → transcript sent to GPT-4o for report generation
7. Agent reads report back to user at chosen speed (1x/1.5x/2x)
8. User edits by voice ("change the TL;DR to...", "remove the part about...")
9. User confirms → report saved to Notion
```

## New API Routes

| Route | Method | Purpose |
|-------|--------|---------|
| `/api/session/create` | POST | Create a new voice session (generates sessionId) |
| `/api/session/[sessionId]/join` | GET | Create LiveKit room, return participant token |
| `/api/session/[sessionId]/state` | GET | Fetch current session state (for resume) |
| `/api/session/[sessionId]/state` | PATCH | Save agent's progress (question answers, transcript) |
| `/api/session/[sessionId]/complete` | POST | Receive final transcript from agent, generate report |
| `/api/session/[sessionId]/report` | GET | Fetch generated report for review |

Routes unchanged:

| Route | Notes |
|-------|-------|
| `/api/generate-report` | Still used to generate structured report from transcript |
| `/api/refine-report` | Still used for post-call report editing |
| `/api/save-to-notion` | Unchanged |
| `/api/transcribe` | Kept as fallback for text-based flow |
| `/api/shame-bot` | Unchanged (upgraded in V5) |

## New Pages

| Route | Purpose |
|-------|---------|
| `/call/[sessionId]` | Voice agent session UI (interview + voice review + voice editing) |
| `/call/[sessionId]/review` | Visual report review fallback (reuses existing report UI) |
| `/call/new` | Start a new voice agent session (requires Clerk auth) |

Existing pages (`/checkin`, `/checkin/text`) remain as alternative input methods.

**Voice session UI (`/call/[sessionId]`):**

- Minimal interface
- Shows: agent avatar/waveform, current question number (1/4), mute button, end call button
- Requires Clerk auth (standard web app login)
- Auto-connects on page load (requests mic permission, then starts)
- When the session ends, transitions to the report review screen

## LiveKit Agent (Python)

The voice agent runs as a separate service using the [LiveKit Agents Framework](https://docs.livekit.io/agents/).

**Key details:**

- Language: Python (LiveKit Agents SDK is Python-first)
- Hosted separately from the Next.js app (e.g., Railway, Fly.io, or a VM)
- Connects to LiveKit Cloud — dispatched automatically when a room is created
- Uses room metadata to receive user context (name, team, weekOf)

**Agent pipeline:**

```python
# Pseudocode
agent = VoicePipelineAgent(
    stt=deepgram.STT(),
    llm=openai.LLM(model="gpt-4o"),
    tts=openai.TTS(voice="alloy"),
)
```

**System prompt (summary):**

> You are Brief, a friendly weekly check-in assistant. You're interviewing {name} for their weekly status update (week of {weekOf}). Ask 4 questions one at a time. Keep it conversational and concise. If an answer is vague, ask one follow-up. After all questions, summarize what you heard and confirm. Then end the session.

**Transcript handling:**

- The agent collects the full conversation transcript
- On session end, it sends the transcript to the Brief backend via `/api/session/[sessionId]/complete`
- The backend processes it through the existing report generation pipeline

## New Environment Variables

| Variable | Purpose | New/Changed/Removed |
|----------|---------|---------------------|
| `LIVEKIT_API_KEY` | LiveKit server API key | New |
| `LIVEKIT_API_SECRET` | LiveKit server API secret | New |
| `LIVEKIT_URL` | LiveKit server WebSocket URL | New |
| `DEEPGRAM_API_KEY` | Deepgram STT API key (if using Deepgram) | New |
| `OPENAI_API_KEY` | Unchanged (used for LLM + TTS + report generation) | - |
| `NOTION_API_KEY` | Unchanged | - |
| `NOTION_DATABASE_ID` | Unchanged | - |
| `NEXT_PUBLIC_CLERK_PUBLISHABLE_KEY` | Unchanged | - |
| `CLERK_SECRET_KEY` | Unchanged | - |
| `SLACK_WEBHOOK_URL` | Unchanged (upgraded in V5) | - |

## Updated User Flow

**Voice agent (primary V3 flow):**

1. User opens Brief web app, logs in via Clerk
2. Taps "Start Check-in" → new voice session created
3. Agent asks 4 questions conversationally (~3 min)
4. Report is generated from conversation
5. Agent reads report back at user's chosen speed (1x / 1.5x / 2x)
6. User edits by voice until satisfied
7. User says "save it" → report saved to Notion

**Direct access (existing flows still work):**

- `/checkin` — voice recording flow (v1)
- `/checkin/text` — text input flow (v1)
